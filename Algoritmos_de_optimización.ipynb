{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Algoritmos-de-optimización.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNvHoCWU361klho7LINuLE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nachovera93/IA-Documents/blob/main/Algoritmos_de_optimizaci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnbAmaSz75hh"
      },
      "source": [
        "#**Algoritmos de Optimización y Learning Rate**\n",
        "\n",
        "<br>\n",
        "\n",
        "Como nosotros podriamos encontrar los mejores pesos que nos ayuden a resolver el problema?\n",
        "\n",
        "<br>\n",
        "\n",
        "####**Optimización**\n",
        "\n",
        "* El objetivo de un algoritmo de optimización es entregar los pasos para minimizar la función de perdida, modificando los pesos del modelo. Estos algoritmos son iterativos, osea que dan pasos a un optimo.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Algoritmo Gradiente Descendente**\n",
        "* Que es el gradiente?\n",
        "  * La derivada nos entrega la pendiente de los pesos, mientras mas empinada(mayor el valor), mayor debe ser el cambio. El gradiente nos indica la dirección en la cual  tenemos que mover los pesos para acercarnos a un optimo.En GD por cada epoca se va dando un paso hacia la optimización.\n",
        "* Pasos pequeños o largos?\n",
        "\n",
        "> **Algoritmo SGD (Stochastic)** \n",
        "\n",
        "  > No buscamos un valor perfecto de nuestro gradiente sino que un aproximado. Aparte, si el dataset que tenemos es mu grande el GD puede ser muy lento, ya que por cada iteración debemos pasar por todo el dataset. El SGD crea un subconjunto de datos(batch), por lo que nos permite acelerar el proceso de convegencia. Con SGD hacemos un paso hacia la optimización por cada batch aleatorio que tengamos. Ej: si tenemos 100 batchs haremos 100 iteraciones por cada epoca.\n",
        "* Momentum\n",
        "* Nesterov\n",
        "* Learning Rate\n",
        "\n",
        "> **Alternativas a SGD**\n",
        "* AdaGrad\n",
        "* Adam\n",
        "\n",
        "> **Adaptar el Learning Rate durante el entrenamiento**\n",
        "* Scheduler\n",
        "* Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICqI1EDT7Yw6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}