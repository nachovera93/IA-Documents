{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grafos de computo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNukt/E1T8dzsqiMWG0Q1k+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nachovera93/IA-Documents/blob/main/Grafos_de_computo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BuRrI0q8q5J"
      },
      "source": [
        "#**Grafos de computo**\n",
        "\n",
        "\n",
        "Esta metodologia nos permitirá estructurar el modelo, la metodologia, el mecanismo y algoritmo de aprendizaje para poder hacerlo escalable y además nos permitira hacer la actualización de los pesos de las redes.\n",
        "Implementaremos un algoritmo de backpropagation\n",
        "\n",
        "<br>\n",
        "\n",
        "![Artificial Neural Network](https://www.bogotobogo.com/python/scikit-learn/images/NeuralNetwork1/NN-with-components-w11-etc.png)\n",
        "\n",
        "\n",
        "###**Que define a una red neuronal?**\n",
        "\n",
        "* Estructura: Numero de capas, numero de neuronas por capa.\n",
        "* La metodologia de calcular pesos $W_ij$, que conecta cada neurona *i* con la  neurona *j*. \n",
        "* La función de activación que modulará la señal que se enviá de una neurona a otra.\n",
        "\n",
        "<br>\n",
        "\n",
        "Los pesos que tendrá la red neuronal establecerá la relación entre la salida y la entrada, por lo que se debe encontrar una cantidad de pesos que caracterizarán el modelo y lograrán el mapeado deseado.\n",
        "\n",
        "<br>\n",
        "\n",
        "###**Perceptrón**\n",
        "\n",
        "<br>\n",
        "\n",
        "![Perceptrón](https://pythonmachinelearning.pro/wp-content/uploads/2017/09/Single-Perceptron.png.webp)\n",
        "\n",
        "* Es la unidad fundamental de una red neuronal.\n",
        "\n",
        "<br>\n",
        "\n",
        "####**Que debo hacer para que el perceptrón aprenda?**\n",
        "\n",
        "Para que el percetrón pueda aprender esa relacion entre la entrada y la salida será fundamentalmente modificar los pesos y asi encontrar un conjunto de pesos tal que establezca esta relación. Por lo tanto entrenar corresponderá a encontrar el mejor conjunto de pesos que establezca una relación entre la salida y la entrada.\n",
        "\n",
        "###**Como podemos entrenar el Perceptrón?**\n",
        "\n",
        "<br>\n",
        "\n",
        "* Como podemos elegir una buena funcion de objetivo?\n",
        "\n",
        "> Esto quiere decir, de que forma podriamos encontrar un criterio de aprendizaje que nos informe como mover estos pesos para encontrar el objetivo final?\n",
        "Para elegir una buena funcion objetivo se deberá encontrar una expresión matematica que cuantifique el error.\n",
        "\n",
        "![Error](https://d3i71xaburhd42.cloudfront.net/a6a3cdee2fa0ddbe3e88b3be2662a06fb2a38304/75-Figure3.12-1.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "Que es el error? será el valor actual que esta botando a la salida comparada con el valor que yo deseo.\n",
        "Por lo que yo debo encontrar el valor mas pequeño de este.\n",
        "\n",
        "\n",
        "* Que caracteristicas debe tener la funcion objetivo?\n",
        "\n",
        "> Que sea convexa ya que eso me garantizará que podre ir moviendome a través de ella hasta encontrar el minimo error.\n",
        "\n",
        "* Que algoritmo de optimización podemos usar para resolver este problema?\n",
        "\n",
        "> El SGD ( Gradiente de descenso estocastico)\n",
        "\n",
        "\n",
        "\n",
        "**Por lo que los pasos de nuestro entrenamiento serán: Forward, Backward and Weights Update.** \n",
        "\n",
        "* Que significa esto?:\n",
        "> Primero tomar por ejemplo una imagen, hacer el paso de forward (pasarla por la red y obtener el resultado final)\n",
        "> Segundo paso será tomar este resultado y compararlo con lo que deberia ser y asi generar el error.\n",
        "> Con el error ya dado haremos el paso de backward, que tomará la cuatificación del error dada por la superficie de la imagen anterior y propagará estos valores de error por la red, por lo que si el valor del error es grande, este al propagarse hacia atrás modificara de gran forma el valor de los pesos.\n",
        "\n",
        "\n",
        "![image.png](https://www.simplilearn.com/ice9/free_resources_article_thumb/symbolic-representation-of-perceptron-learning-rule.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "$$wi ← wi + ∆wi$$\n",
        "<br>\n",
        "$$∆wi = −η · \\frac{∂E\\vec{w}}{∂wi}\\ $$\n",
        "<br>\n",
        "$$E(\\vec{w}) = \\frac{1}{2} · (t_d - o_d)^2$$\n",
        "<br>\n",
        "$$\\frac{∂E(\\vec{w})}{∂wi} = -\\sum_{d∈D}(t_d - o_d) · o_d · (1 − o_d ) · x_i,_d\\$$ \n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Para actualizar los pesos tendre un valor inicial $w_i$ que será el peso actual y le sumare $∆w_i$ y lo actualizaré.\n",
        "Este $w_i$ se basa en que tan fuerte es el gradiente, por lo que es igual al gradiente de la funcion de error que definimos con respecto al peso en cuestión.\n",
        "Entonces por cada iteracion el $∆w_i$ ira decreciendo.\n",
        "\n",
        "$t_d$ (target d) = corresponde a la salida esperada y $o_d$ corresponde a la salida real \n",
        "\n",
        "<br>\n",
        "\n",
        "Acomodamos un poco la ecuacion para trabajar con vectores, quedaría todo de la siguiente forma:\n",
        "\n",
        "$$\\vec{w} ← \\vec{w}+ ∆\\vec{w}$$\n",
        "<br>\n",
        "$$∆\\vec{w} = \\eta · \\nabla E(\\vec{w}) $$\n",
        "<br>\n",
        "$$\\nabla E(\\vec{w}) = [\\frac{∂E\\vec{w}}{∂w_0},\\frac{∂E\\vec{w}}{∂w_1},...,\\frac{∂E\\vec{w}}{∂w_n}]$$\n",
        "\n",
        "\n",
        "###**BackPropagation**\n",
        "\n",
        "Backpropagation nos permitirá ver cual es la responsabilidad de cada peso en el error de la red, y así veremos en que valor tenemos que variar cada peso para que la red cada vez aprendá más.\n",
        "\n",
        "* Esta basado en la regla de la cadena de forma recursiva.\n",
        "* Puede ser usado en redes de arbitrario tamaño.\n",
        "* Puede ser usado por cualquier tipo de red con funciónes diferenciables.\n",
        "* Pero no esta garantizado un buen rendimiento o llegar a un buen minimo local o global.\n",
        "\n",
        "<br>\n",
        "\n",
        "![Calculo de errores](https://www.interactivechaos.com/sites/default/files/inline-images/tutorial_ml_096.png)\n",
        "\n",
        "<br>\n",
        "$$x_i · w(L-1) = a(L-1)$$ \n",
        "<br>\n",
        "$$E\\vec{w}= (y-(\\sum_{}w_i·x_i+b))^2$$\n",
        "<br>\n",
        "$$\\frac{∂E(\\vec{w})}{∂wi}=\\frac{∂(a\\vec{L})}\n",
        "{∂aL_-1}·\\frac{∂({aL_-1})}{∂(wL-1)}$$\n",
        "\n",
        "\n",
        "\n",
        "Esta imagen y ecuación nos muestra como el error se va propagando hacia atrás utilizando derivadas locales y multiplicaciones.\n",
        "\n",
        "<br>\n",
        "\n",
        "![activations](https://miro.medium.com/max/771/1*DcLWqOojI1b9jzQaLibUkQ.png)\n",
        "\n",
        "Aqui se lográ apreciar como la propragación del gradiente de error(L) se va multiplicando con las derivadas locales para podr propagar a este, en el paso de forward se calculan las derivadas locales y al hacer backpropagation estas se van multiplicando con este gradiente.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[1]https://towardsdatascience.com/back-propagation-simplified-218430e21ad0"
      ]
    }
  ]
}