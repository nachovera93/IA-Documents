{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Herramientas-Deep-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOuhg74XLt0e9PsWoRR2Mek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nachovera93/IA-Documents/blob/main/Herramientas_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oT0xBeP_0Ur"
      },
      "source": [
        "##**Funciones de perdida**\n",
        "<br>\n",
        "\n",
        "Cuantifica que tan bien estan nuestras etiquetas de clase predichas con respecto a nuestras etiquetas de verdad, el objetivo es minimizar la función de perdida.\n",
        "<br>\n",
        "\n",
        "Mientras mas pequeña la pérdida, mejor será el trabajo del clasificador para modelar la relación entre los datos de entrada y etiquetas de clase de salida (aunque hay un punto en el que podemos sobreajustar nuestro modelo, modelando el\n",
        "datos de entrenamiento demasiado de cerca, y nuestro modelo pierde la capacidad de generalizar.\n",
        "\n",
        "<br>\n",
        "\n",
        "Como sabemos, para probar la precisión de nuestro modelo de clasificación. necesitamos tunear nuestros parametros de los pesos W o del vector bias. Estos los optimizamos con tecnicas como el SGD. Idealmente. nuestra perdida debería mostrar un decrecimiento en el tiempo.\n",
        "\n",
        "<br>\n",
        "\n",
        "![FunciondePerdida](https://drive.google.com/uc?export=view&id=1IVV8rnNJg65sfCd0o5B7_Sgs27wndXbP)\n",
        "\n",
        "<br>\n",
        "\n",
        "f* = Modelo optimo\n",
        "<br>\n",
        "\n",
        "En el caso general aproximaremos el modelo optimo al mejor modelo en el set de training. \n",
        "El mejor modelo del set de training será aquel que en el espacio hipotesis (f E H) minimice nuestra funcion de perdida en promedio.\n",
        "\n",
        "<br>\n",
        "\n",
        "En el caso supervisado, la diferencia viene dada porque la función  de perdida tambien dependerá de la clase o label y no solamente del dato.\n",
        "\n",
        "<br>\n",
        "\n",
        "###**Función de perdida MSE**\n",
        "<br>\n",
        "\n",
        "MSE(Error cuadratico medio)\n",
        "<br>\n",
        "Será el promedio entre la diferencia entre la predicción o clase predicha y el objetivo real al cuadrado.\n",
        "\n",
        "<br>\n",
        "\n",
        "![MSE](https://drive.google.com/uc?export=view&id=1l_XBp514M7Z39rrHJTc5QVy_A4JMR5Xj)\n",
        "\n",
        "<br>\n",
        "\n",
        "El termino al cuadrado penaliza mas nuestra perdida al elevar al cuadrado la salida (crecimiento cuadratico de la salida).\n",
        "<br>\n",
        "\n",
        "El que tiene mejor MSE es el modelo de la derecha.\n",
        "\n",
        "<br>\n",
        "\n",
        "###**Cross-Entropy y Softmax**\n",
        "\n",
        "<br>\n",
        "\n",
        "El clasificador softmax nos da probabilidades por cada clase mientras que las funciones de perdida como MSE nos dan un margen.\n",
        "Es mucho mas facil interpretar probabilidades que margenes.\n",
        "Además, para conjuntos de datos como ImageNet, observamos la precisión de rango 5 de CNNs para ver si la verdadera etiqueta de clase existe en el top-5 de\n",
        "predicciones y la probabilidad asociada con cada etiqueta es una buena propiedad.\n",
        "\n",
        "<br>\n",
        "\n",
        "####**Cross-Entropy**\n",
        "\n",
        "<br>\n",
        "\n",
        "![Cross-entropy](https://drive.google.com/uc?export=view&id=1cA9tg8txnPYIdAuFuaKPI8O39GjI6G0M)\n",
        "\n",
        "![SoftMax](https://drive.google.com/uc?export=view&id=1RkvqM4H4OUdTM-puWJWdQT6zqJBlDJ2e)\n",
        "\n",
        "![Result](https://drive.google.com/uc?export=view&id=1F6BURC3CLEb6Yhm7cauC-cZrqMOr0dN1)\n",
        "\n",
        "![Cross-Soft](https://drive.google.com/uc?export=view&id=1aytgT_oDfWG-9dn0u5j4CIb9TWyHEqb1)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Ahora, que pasa si el modelo se veía bien en teoría, pero no aprende? o aprende una correlación incorrecta? no generaliza? sobreajuste? o simplemente no alcanza el rendimiento que se quisiera?\n",
        "\n",
        "<br>\n",
        "\n",
        "###**Regularización**\n",
        "\n",
        "<br>\n",
        "\n",
        "\"Muchas estrategias utilizadas en el aprendizaje automático están diseñadas explícitamente para reducir el\n",
        "error de prueba, posiblemente a expensas de un mayor error de entrenamiento. Estas estrategias son\n",
        "conocido colectivamente como regularización \". - Goodfellow et al.\n",
        "\n",
        "<br>\n",
        "\n",
        "Como vamos a elegir un set de parametros que nos ayuden a que nuestro modelo generalize bien? la respuesta es regularización.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}