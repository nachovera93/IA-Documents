{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "authorship_tag": "ABX9TyMoRQozWtKYefQEPLOHaTPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nachovera93/IA-Documents/blob/main/Intro-PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsYLLYiiK1ZA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5kbplkjBdeg"
      },
      "source": [
        "##**Introducción a PyTorch**\n",
        "\n",
        "PyTorch es una libreria para programas de python que facilita los proyectos de Deep Learning. PyTorch nos da un tipo de datos, el **Tensor** que son arrays multidimensionales para contener números, vectores, matrices, o matrices en general. Además, proporciona funciones para operar sobre ellos. Se puede\n",
        "programar con ellos de forma incremental y, si queremos, interactivamente, muy parecido a Numpy.\n",
        "<br>\n",
        "\n",
        "En segundo lugar, PyTorch proporciona instalaciones que admiten la optimización numérica en\n",
        "Expresiones matemáticas genéricas, que el aprendizaje profundo utiliza para el entrenamiento. \n",
        "<br>\n",
        "\n",
        "PyTorch\n",
        "ha sido equipado con un tiempo de ejecución C ++ de alto rendimiento que se puede utilizar para implementar\n",
        "modelos para inferencia sin depender de Python, y se puede utilizar para diseñar y\n",
        "entrenar modelos en C ++. También ha crecido vínculos con otros idiomas y una interface para implementar en dispositivos móviles. Estas características permiten aprovechar\n",
        "la flexibilidad de PyTorch y al mismo tiempo llevar nuestras aplicaciones donde un script de Python completo\n",
        " sería difícil de conseguir.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "En el momento de la primera versión beta de PyTorch:\n",
        "* Theano y TensorFlow fueron las principales bibliotecas de bajo nivel, trabajando con un\n",
        "modelo que hizo que el usuario definiera un gráfico computacional y luego lo ejecutara.\n",
        "* Lasagne y Keras fueron envoltorios de alto nivel alrededor de Theano, con envoltura de Keras a TensorFlow y CNTK también.\n",
        "* Caffe, Chainer, DyNet, Torch (el precursor de PyTorch basado en Lua), MXNet,\n",
        "CNTK, DL4J y otros llenaron varios nichos en el ecosistema.\n",
        "<br>\n",
        "\n",
        "En los aproximadamente dos años que siguieron, el panorama cambió drásticamente. La comunidad consolidada en gran medida detrás de PyTorch o TensorFlow, con la adopción\n",
        "de otras bibliotecas menguando, excepto aquellas que llenan nichos específicos. En una palabra:\n",
        "* Theano, uno de los primeros marcos de aprendizaje profundo, ha dejado de desarrollarse activamente.\n",
        "* TensorFlow:\n",
        ">Consumío Keras por completo, promoviéndolo a una API de primera clase\n",
        "<br>\n",
        "> Proporcionó un \"modo inmediato (eager)\" de ejecución que es algo similar a cómo PyTorch aborda la computación.\n",
        "<br>\n",
        "\n",
        "\n",
        "* JAX, una biblioteca de Google que se desarrolló independientemente de TensorFlow,\n",
        "ha comenzado a ganar tracción como un equivalente de NumPy con GPU, autograd y Capacidades JIT.\n",
        "\n",
        "> PyTorch:\n",
        "- Consumío Caffe2 para su backend\n",
        "- Reemplazó la mayor parte del código de bajo nivel reutilizado del proyecto Torch basado en Lua\n",
        "- Se agregó soporte para ONNX, una descripción del modelo independiente del proveedor y formato de intercambio\n",
        "- Se agregó un tiempo de ejecución de \"modo gráfico\" de ejecución retardada llamado *TorchScript*\n",
        "- Versión 1.0 publicada\n",
        "- Reemplazó CNTK y Chainer como el marco de elección por sus respectivos\n",
        "patrocinadores corporativos.\n",
        "<br>\n",
        "\n",
        "TensorFlow tiene una sólida canalización de producción, una extensa comunidad de\n",
        "nidad y mentes compartidas masivas. PyTorch ha hecho grandes avances con la investigación y\n",
        "comunidades de enseñanza, gracias a su facilidad de uso, y ha cobrado impulso desde que investigadores y graduados capacitan a estudiantes y se trasladan a la industria. También se ha construido en términos de soluciones de producción. Curiosamente, con la llegada de TorchScript\n",
        "y modo \"eager\", tanto PyTorch como TensorFlow han visto cómo sus conjuntos de funciones comienzan a\n",
        "convergen con los demás, aunque la presentación de estas características y la experiencia sigue siendo bastante diferente entre los dos.\n",
        "\n",
        "<br>\n",
        "\n",
        "###**Que necesita un proyecto de Deep Learning por PyTorch**\n",
        "\n",
        "Primero, PyTorch tiene el \"Py\" como en Python, pero hay mucho código que no es Python. En realidad, por razones de rendimiento, la mayor parte de PyTorch está escrito en C ++ y CUDA, un lenguaje similar a C ++ de NVIDIA que se puede compilar para ejecutarse con un paralelismo masivo en las GPU. Hay formas de correr PyTorch directamente desde C ++.\n",
        "\n",
        "\n",
        "Teniendo tensores y la biblioteca estándar de tensor habilitada para autogrado, PyTorch puede utilizarse para física, renderizado, optimización, simulación, modelado y más. Pero PyTorch es ante todo una biblioteca de aprendizaje profundo y proporciona todos los componentes básicos necesarios para construir redes neuronales y entrenarlas. La siguiente figura muestra una configuración estándar que carga datos, entrena un modelo y luego implementa ese\n",
        "modelo a producción\n",
        "\n",
        "\n",
        "![Structure of PyTorch](https://miro.medium.com/max/2566/1*V4YmDrMt5aZ975suteX98w.png)\n",
        "\n",
        "El nucleo de PyTorch para construir redes neuronales esta licalizado en *torch.nn*, que provee una comúún red neuronal con capas y demases arquitecturas como Fully connected, CNNs, funciones de activacion, funciones de perdida, etc.\n",
        "\n",
        "Primero necesitamos obtener físicamente los datos, la mayoría\n",
        "a menudo de algún tipo de almacenamiento como fuente de datos. Entonces necesitamos convertir cada muestra de nuestros datos a algo que PyTorch realmente puede manejar: tensores. Este puente entre nuestros datos personalizados (en cualquier formato que sea) y un estándarizado tensor de PyTorch es la clase de conjunto de datos que PyTorch proporciona en *torch.utils.data*. Como este\n",
        "proceso es tremendamente diferente de un problema a otro, tendremos que implementar estos datos provienentes de nosotros mismos.\n",
        "\n",
        "Como el almacenamiento de datos suele ser lento, en particular debido a la latencia de acceso, queremos paralelizar la carga de datos. PyTorch proporciona fácilmente toda esa magia en la clase *DataLoader*. Sus instancias pueden generar procesos secundarios para cargar datos desde un dataset en segundo plano para que esté listo y esperando el ciclo de entrenamiento tan pronto como loop pueda usarlo.\n",
        "\n",
        "En cada paso del ciclo de entrenamiento, evaluamos nuestro modelo en las muestras que obtuvimos desde el cargador de datos. Luego comparamos los resultados de nuestro modelo con los resultados deseados(los objetivos) usando algún criterio o función de pérdida. Así como ofrece los componentes a partir del cual construir el modelo, PyTorch también tiene una variedad de funciones de pérdida. Ellos se proporcionan en *torch.nn*. Después de haber comparado nuestro resultado real\n",
        "pone al ideal con las funciones de pérdida, necesitamos empujar un poco al modelo para mover sus salidas para parecerse mejor al objetivo. Como se mencionó anteriormente, aquí es donde PyTorch\n",
        "entra en modo autogrado; pero también necesitamos un optimizador que realice las actualizaciones, y eso es lo que PyTorch nos ofrece en *torch.optim*\n",
        "\n",
        "<br>\n",
        "\n",
        "##**Pequeño ejemplo de clasificador de imagenes con PyTorch**\n",
        "\n",
        "\n",
        "![Example](https://miro.medium.com/max/2048/0*L_7h9wNLMWzDDr7M.png)\n",
        "\n",
        "La imagen de input primero será procesada dentro de una instancia de un tensor clase *torch.Tensor*. Es una imagen RGB con altura y ancho, por lo que este tensor tendrá 3 dimensiones: los 3 canales de colores, y dos imágenes espaciales con dimensiones de un tamaño específico. Luego nuestro modelo pasará a una red pre-entrenada que obtendrá puntajes para cada clase. El puntaje mayor corresponderá a la clase mas parecida de acuerdo a los pesos. Los outputs estan contenidos en *torch.Tensor*.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfpfdw7usACr",
        "outputId": "d44eb2f7-53ee-48ca-cf94-e7012c9947ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torchvision import models\n",
        "dir(models)  #devuelve una lista de nombres de todos los atributos válidos y atributos básicos\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AlexNet',\n",
              " 'DenseNet',\n",
              " 'GoogLeNet',\n",
              " 'GoogLeNetOutputs',\n",
              " 'Inception3',\n",
              " 'InceptionOutputs',\n",
              " 'MNASNet',\n",
              " 'MobileNetV2',\n",
              " 'ResNet',\n",
              " 'ShuffleNetV2',\n",
              " 'SqueezeNet',\n",
              " 'VGG',\n",
              " '_GoogLeNetOutputs',\n",
              " '_InceptionOutputs',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_utils',\n",
              " 'alexnet',\n",
              " 'densenet',\n",
              " 'densenet121',\n",
              " 'densenet161',\n",
              " 'densenet169',\n",
              " 'densenet201',\n",
              " 'detection',\n",
              " 'googlenet',\n",
              " 'inception',\n",
              " 'inception_v3',\n",
              " 'mnasnet',\n",
              " 'mnasnet0_5',\n",
              " 'mnasnet0_75',\n",
              " 'mnasnet1_0',\n",
              " 'mnasnet1_3',\n",
              " 'mobilenet',\n",
              " 'mobilenet_v2',\n",
              " 'quantization',\n",
              " 'resnet',\n",
              " 'resnet101',\n",
              " 'resnet152',\n",
              " 'resnet18',\n",
              " 'resnet34',\n",
              " 'resnet50',\n",
              " 'resnext101_32x8d',\n",
              " 'resnext50_32x4d',\n",
              " 'segmentation',\n",
              " 'shufflenet_v2_x0_5',\n",
              " 'shufflenet_v2_x1_0',\n",
              " 'shufflenet_v2_x1_5',\n",
              " 'shufflenet_v2_x2_0',\n",
              " 'shufflenetv2',\n",
              " 'squeezenet',\n",
              " 'squeezenet1_0',\n",
              " 'squeezenet1_1',\n",
              " 'utils',\n",
              " 'vgg',\n",
              " 'vgg11',\n",
              " 'vgg11_bn',\n",
              " 'vgg13',\n",
              " 'vgg13_bn',\n",
              " 'vgg16',\n",
              " 'vgg16_bn',\n",
              " 'vgg19',\n",
              " 'vgg19_bn',\n",
              " 'video',\n",
              " 'wide_resnet101_2',\n",
              " 'wide_resnet50_2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6h94DxqsyPS"
      },
      "source": [
        "Los nombres en mayúscula se refieren a clases de Python que implementan una serie de populares\n",
        "modelos. Se diferencian en su arquitectura, es decir, en la disposición de las operaciones.\n",
        "que ocurre entre la entrada y la salida. Los nombres en minúsculas son convenientes\n",
        "funciones que devuelven modelos instanciados de esas clases, a veces con diferentes\n",
        "conjuntos de parámetros. Por ejemplo, resnet101 devuelve una instancia de ResNet con 101 capas,\n",
        "resnet18 tiene 18 capas y así sucesivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2fB50KtBqmn"
      },
      "source": [
        "##**Bibliografia**\n",
        "[1]Deep Learning with PyTorch, Eli Stevens,Luca Atiaga,Thomas Viehman."
      ]
    }
  ]
}