{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfomers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP97aiovt5HVM6zgWd5+dxc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nachovera93/IA-Documents/blob/main/Transfomers-beta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCKcD1CYiHhE"
      },
      "source": [
        "<h1><b>Transformers</h1></b>\n",
        "\n",
        "Los transformadores están arrasando en el mundo del procesamiento del lenguaje natural. Se utilizan en muchas aplicaciones como traducción de lenguaje automático, chatbots conversacionales e incluso para impulsar mejores motores de búsqueda.\n",
        "\n",
        "<br>\n",
        "\n",
        "¿Cómo funcionan? ¿Por qué han superado al anterior rey de los problemas de secuencia, como las redes neuronales recurrentes, GRU y LSTM?\n",
        "\n",
        "<h2><b>Mecanismo de atención</h2></b>\n",
        "\n",
        "Para comprender los transformadores, primero debemos comprender el mecanismo de atención. El mecanismo de atención permite que los transformadores tengan una memoria a muy largo plazo. Un modelo de transformador puede \"atender\" o \"concentrarse\" en todos los tokens anteriores que se han generado.\n",
        "\n",
        "A medida que el modelo genera el texto palabra por palabra, puede \"atender\" o \"concentrarse\" en palabras que son relevantes para la palabra generada. La capacidad de saber qué palabras atender también se aprende durante el entrenamiento a través del backpropagation. \n",
        "\n",
        "Las redes neuronales recurrentes (RNN) también son capaces de buscar entradas anteriores también. Pero el poder del mecanismo de atención es que no sufre de memoria a corto plazo. Los RNN tienen una ventana más corta para hacer referencia, por lo que cuando la historia se alarga, los RNN no pueden acceder a las palabras generadas anteriormente en la secuencia. \n",
        "\n",
        "El mecanismo de atención, en teoría, y con suficientes recursos informáticos, tiene una ventana infinita desde la que hacer referencia, por lo que es capaz de utilizar todo el contexto de la historia al generar el texto. \n",
        "\n",
        "<br>\n",
        "\n",
        "<h2><b>La atención es todo lo que necesitas:</h2></b>\n",
        "\n",
        "El poder del mecanismo de atención se demostró en el artículo \"La atención es todo lo que necesitas \", donde los autores introdujeron una nueva red neuronal novedosa llamada Transformers, que es una arquitectura de tipo codificador-decodificador basada en la atención. \n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://miro.medium.com/max/1090/1*HunNdlTmoPj8EKpl-jqvBA.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "En un nivel alto, el codificador mapea una secuencia de entrada en una representación continua abstracta que contiene toda la información aprendida de esa entrada. Luego, el decodificador toma esa representación continua y paso a paso genera una única salida mientras también se alimenta la salida anterior. \n",
        "\n",
        "<br>\n",
        "\n",
        "<h3><b>Input Embedding</h3></b>\n",
        "\n",
        "<br>\n",
        "\n",
        "Es un algoritmo que convierte el texto en una serie de vectores o tokens. Las redes neuronales aprenden a través de números, por lo que cada palabra se asigna a un vector con valores continuos para representar esa palabra. Como la secuencia se procesa en paralelo será necesario indicarle a la red el orden en que se encuentran las palabras dentro del texto. Esto se logra con el Codificador posicional.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://miro.medium.com/max/291/0*6MnniQMOBPu4kFq3.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "<h3><b>Positional Encoding</h3></b>\n",
        "\n",
        "<br>\n",
        "\n",
        "El siguiente paso es inyectar la información posicionalmente en los embeddings. <br>\n",
        "Debido a que el codificador del transformador no tiene recurrencia como las redes neuronales recurrentes, debemos agregar alguna información sobre las posiciones en las incorporaciones de entrada. Esto se hace mediante codificación posicional y como se explica en el paper se usaron funciones senoidales y cosenoidales \n",
        "\n",
        "<br>\n",
        "\n",
        "$\\ PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) $\n",
        "\n",
        "$\\ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}}) $\n",
        "\n",
        "<br>\n",
        "\n",
        "Para cada índice impar en el vector de entrada, crea un vector usando la función cos. Para cada índice par, crea un vector usando la función sin. Luego se agregan esos vectores a sus incrustaciones de entrada correspondientes. Esto le da a la red información sobre la posición de cada vector. Cada vector generado tendrá un patrón numerico unico con la información de la posición.\n",
        "\n",
        "<br>\n",
        "\n",
        "<h3><b>Encoded Layer</h3></b>\n",
        "\n",
        "<br>\n",
        "\n",
        "El enconder esta compuesto por Nx=6 capas identicas (no comparten pesos). Cada capa tiene 2 sub-modulos. La primera es un mecanismo multi-head self-attention. La segunda es una red neuronal positional fully connected feed-froward. También hay conexiones residuales alrededor de cada una de las dos subcapas seguidas de una normalización de capa. El trabajo de las capas de codificadores es mapear todas las secuencias de entrada en una representación continua abstracta que contiene la información aprendida para toda esa secuencia. \n",
        "Aquí comenzamos a ver una propiedad clave del Transformer, que es que la palabra en cada posición fluye a través de su propia ruta en el codificador. Hay dependencias entre estos caminos en la capa de auto-atención. Sin embargo, la capa de retroalimentación no tiene esas dependencias y, por lo tanto, las diversas rutas se pueden ejecutar en paralelo mientras fluyen a través de la capa de retroalimentación. \n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://miro.medium.com/max/564/0*gxx0-uUpZfAmKmPa.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2><b>Multi-Headed Attention</h2></b>\n",
        "\n",
        "<br>\n",
        "\n",
        "Este se podría decir que es el bloque mas importante de toda la red. Tiene un mecanismo de auto-atención permite a los modelos asociar cada palabra de la entrada con otras palabras. Por ende, esta se encargara de analizar la totalidad de la secuencia de entrada(que la red procesa de manera simultanea) y de encontrar relaciones entre varías palabras de esta secuencia. Entonces, este bloque buscará expresar numericamente las relaciónes que existen entre palabras, indicando asi cuales son los elementos del texto a los que debería prestar atención. Para lograr esto primero que nada se llenan los tokens simultaneamente a 3 pequeñas redes neuronales Query, Key and Value.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://dair.ai/images/summary-attention-is-all-you-need/attentions.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2><b>Multi-atención al detalle</h2></b>\n",
        "\n",
        "Para lograr la atención propia, primero alimentamos la entrada en 3 capas distintas completamente conectadas para crear los vectores de consulta, clave y valor.\n",
        "Estas estan entrenadas para calcular los vectores Queries, Keys y Values. Que son simplemente 3 representaciones alternativas de los tokens originales. No tienen que ser más pequeños, esta es una elección de arquitectura para hacer que el cálculo de la atención de múltiples cabezas (en su mayoría) sea constante. \n",
        "\n",
        "<br>\n",
        "\n",
        "![](http://jalammar.github.io/images/t/transformer_self_attention_vectors.png)\n",
        "ref: http://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "<br>\n",
        "\n",
        "Se puede observar que estos vectores sonmas pequeños que el vector de inscrutación. Su dimensionalidad es de 64, mientras que los vectores de entrada/salida de inscrustración y codificador tienen una dimensionalidad de 512. \n",
        "\n",
        "<br>\n",
        "\n",
        "Ejemplo: Al multiplicar $X_1$ por la matriz de pesos $W^Q$ se produce q1, el vector de consulta asociado a esa palabra.  \n",
        "\n",
        "<br>\n",
        "\n",
        "<h2><b>Producto escalar entre consulta y clave (MatMul Q&K)</h2></b>\n",
        "\n",
        "<br>\n",
        "\n",
        "El segundo paso para calcular self-atenttion es calcular una puntuación. Supongamos que estamos calculando la atención propia para la primera palabra de este ejemplo, \"Pensamiento\". Necesitamos calificar cada palabra de la oración de entrada contra esta palabra. La puntuación determina cuánto enfoque poner en otras partes de la oración de entrada cuando codificamos una palabra en una posición determinada.\n",
        "\n",
        "<br>\n",
        "\n",
        "Entonces, después de alimentar la consulta, la clave y el vector de valor a través de una capa lineal, las consultas y las claves se someten a una multiplicación de matriz de producto escalar para producir una matriz de puntuación.\n",
        "\n",
        "<br>\n",
        "\n",
        "La entrada consta de consultas y claves de dimension $d_k$ y valores de dimension $d_v$. Calculamos los productos escalares de la consulta con todas las claves, dividimos cada uno por $√d_k$ (8, la raiz cuadrada de la dimensión delos vectores Keys) y aplicamos una función softmax para obtener los pesos de los valores. En la práctica, se calcula la función de atención en un conjunto de consultas simultáneamente, empaquetadas juntas en una matriz Q. Las claves y los valores también se empaquetan juntos en matrices K y V. Calculamos la matriz de salidas como:\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://miro.medium.com/max/469/1*GsLQLch51d7excmuAi4UzQ.png)\n",
        "ref: https://blogs.oracle.com/datascience/multi-head-self-attention-in-nlp\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "$ Attention(Q,K,V) = softmax(\\frac{QK^T}{√d_k})V$\n",
        "\n",
        "<br>\n",
        "\n",
        "La matriz de puntuación determina cuánto debe centrarse una palabra en otras palabras. Entonces, cada palabra tendrá una puntuación que se corresponde con otras palabras en el intervalo de tiempo. Cuanto mayor sea la puntuación, más concentración. Así es como se asignan las consultas a las claves. \n",
        "\n",
        "<br>\n",
        "\n",
        "![](http://jalammar.github.io/images/t/self-attention-output.png)\n",
        "ref: http://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2><b>Reducción y Softmax </h2></b>\n",
        "\n",
        "<br>\n",
        "\n",
        "Luego, las puntuaciones se reducen al dividirse por la raíz cuadrada de la dimensión de la consulta y la clave. Esto es para permitir gradientes más estables, ya que la multiplicación de valores puede tener efectos explosivos.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "A continuación, toma el softmax del puntaje escalado para obtener los pesos de atención, lo que le da valores de probabilidad entre 0 y 1. Al hacer un softmax, los puntajes más altos aumentan y los puntajes más bajos se deprimen. Esto permite que el modelo tenga más confianza en las palabras a las que debe asistir. \n",
        "Finalmente, dado que estamos tratando con matrices, podemos concatenar los pasos en una fórmula final para calcular los resultados de la capa de auto-atención.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://miro.medium.com/max/1838/0*EctVTBob9ZLJN2nC.png)\n",
        "ref : http://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2><b>Multiplicar la salida de Softmax con el vector Values</h2></b>\n",
        "\n",
        "<br>\n",
        "\n",
        "Luego, toma los pesos de atención y los multiplica por su vector de Values para obtener un vector de salida. Los puntajes de softmax más altos mantendrán el valor de las palabras que el modelo aprende. Las puntuaciones más bajas ahogarán las palabras irrelevantes. Luego, alimenta la salida de eso en una capa lineal para procesar. \n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<h2><b>Multi-Head Attention</h2></b>\n",
        "\n",
        "<br>\n",
        "\n",
        "Como se describe en el paper, en lugar de realizar una única función de atención con Keys, Values y Querys de dimensiones de modelo, nos pareció beneficioso proyectar linealmente las queries, keys y values de valores con h diferentes proyecciones lineales aprendidas para $d_k, d_k$ y $d_v$, respectivamente (Se usarón 8 capas de atención en paralelo). En cada una de estas versiones proyectadas de queries, keys y values realizamos la función de atención en paralelo, produciendo valores de salida $d_v$-dimensionales. Estos se concatenan y una vez más se proyectan, lo que da como resultado los valores finales, como se describe en la siguiente figura:\n",
        "\n",
        "<br>\n",
        "\n",
        "![](http://jalammar.github.io/images/t/transformer_attention_heads_z.png)\n",
        " ref : http://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "<br>\n",
        "\n",
        "Pero la capa de retroalimentación espera solo una matriz(un vector para cada palabra). Por lo que concatenizamos las matrices y luego las multiplicamos por una matriz de pesos adicionales $W^o$, para asi producir la salida de la capa. Estas salidass corresponden a un vector por cada token de entrada.\n",
        "\n",
        "![](http://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png)\n",
        "ref : http://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "Con eso concluye el cálculo de la auto-atención. El vector resultante es uno que podemos enviar a la red neuronal Feed-Forward. \n",
        "\n",
        "<br>\n",
        "\n",
        "Pero antes de pasar a la Feed-forward tenemos un bloque residual, que como sabemos se utilizan para redes profundas con muchas capas, con la finalidad de que la informacion no se vaya degradando y dificultando el entrenamiento y por ende el desempeño de la red.\n",
        "Por ende a este bloque entran la entrada que hubo en el bloque atencional con su salida, estas se suman y luego se normalizan con la escala adecuada requerida por el siguiente bloque\n",
        "\n",
        "![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)\n",
        "ref: http://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2><b>Feed Forward</h2></b>\n",
        "\n",
        "<br>\n",
        "\n",
        "Esta red procesará todos los datos en paralelo de cada capa en el codificador, la cual es aplicada pra cada posición separadamente e identicamente. Esta consiste en 2 transformaciones lineales con activaciones ReLU entremedio.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTIPKcNfiDfl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}