{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interpretabilidad.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0SUYSXJRocprhiyZfrVuI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nachovera93/IA-Documents/blob/main/Interpretabilidad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR02fBwEiz_q"
      },
      "source": [
        "##**Interpretabilidad**\n",
        "\n",
        "<br>\n",
        "\n",
        "La interpretabilidad en este contexto sería la capacidad de poder observar en que se esta fijando la red para generar ejemplos.\n",
        "\n",
        "* Visualización de caracteristicas responde las preguntas de la red, o que esta mirando la red para generar ejemplos.\n",
        "* Atribuciones estudia que parte de un ejemplo es responsable de la activación de una parte de la red.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "###**Feature Visualization**\n",
        "\n",
        "<br>\n",
        "\n",
        "Las redes son diferenciables con respecto a sus inputs.\n",
        "* Podemos generar una imagen que maximiza la parte deseada de la red.\n",
        "\n",
        "<br>\n",
        "\n",
        "![1](https://drive.google.com/uc?export=view&id=1jpLZAQk5umsL3jX5E72IUT8BZHmjqURI)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "![2](https://drive.google.com/uc?export=view&id=1We7U1zE5adxExBT1TMzF9NUw22szuhIM)\n",
        "\n",
        "<br>\n",
        "\n",
        "Podemos buscar varios tipos de entrada, de una neurona, canal, capa, clase, etc.\n",
        "\n",
        "<br>\n",
        "\n",
        "![3](https://drive.google.com/uc?export=view&id=1I-TPQlu5wHgIHMS4uh-ZiA2TI5LDi9JZ)\n",
        "\n",
        "<br>\n",
        "\n",
        "Podemos ocupar los logits o la probabilidad de la clase para optimizar, pero si se optimiza el logits se obtiene una mejor imagen:\n",
        "\n",
        "\n",
        "![4](https://drive.google.com/uc?export=view&id=1os_mKx12SlHLMrnDmdznS-3OhIIeII7h)\n",
        "\n",
        "<br>\n",
        "\n",
        "Pero en la realidad, la optimización generá ruido en las imagenes.\n",
        "\n",
        "<br>\n",
        "\n",
        "![5](https://drive.google.com/uc?export=view&id=17_6et8MjizFtZH76GkrwtV02LAdd_-E7)\n",
        "\n",
        "<br>\n",
        "\n",
        "Por que pasa esto? ya que las capas con stride mayor a 1 generan patrones de \"checkerboard\" o tablero de damas en el gradiente cuando la propagación va a través de el.\n",
        "\n",
        "<br>\n",
        "\n",
        "![6](https://drive.google.com/uc?export=view&id=1WnxRaxd_LogNZEa4BJ6TOCBXu0-1msDQ)\n",
        "\n",
        "<br>\n",
        "\n",
        "Por lo que podemos agregar una regularización para generar imagenes mas naturales.\n",
        "\n",
        "* Penalización de frecuencia\n",
        "* Robustez de la transformación\n",
        "\n",
        "<br>\n",
        "\n",
        "Penalización de frecuencia:\n",
        "\n",
        "<br>\n",
        "\n",
        "![7](https://drive.google.com/uc?export=view&id=1DXwV3vKzPoGHYmLI6yIN4RyIQoIFaxpv)\n",
        "\n",
        "<br>\n",
        "\n",
        "Transformación a la robustez(mas resistente a la traslación):\n",
        "\n",
        "<br>\n",
        "![8](https://drive.google.com/uc?export=view&id=11cPAZ9OENbV2AMMI-wRE-K_LN4rjDTfN)\n",
        "\n",
        "<br>\n",
        "\n",
        "Imagenes naturales cambiando la distancia metrica y la parametrización del input tambien ayudan.\n",
        "\n",
        "<br>\n",
        "\n",
        "![9](https://drive.google.com/uc?export=view&id=1wlTZEDXI9UFC-4rl40tDhkwtGNqPujPN)\n",
        "\n",
        "<br>\n",
        "\n",
        "Al juntar el learning rate, el espacio descorrelacionado y la transformación de robustez e debería obtener una imagen mas facil de interpretar.\n",
        "\n",
        "<br>\n",
        "\n",
        "![10](https://drive.google.com/uc?export=view&id=1CaG1ZdIiWWysRNefPl3XLOmKsXD_p4Ge)\n",
        "\n",
        "<br>\n",
        "\n",
        "En resumen, si queremos observar que aprendio una parte de la red en la que tomamos ruido, le aplicamos estas restricciones y realizamos un proceso de optimización para encontrar la imagen que maximiza la activación en la parte de la red que nos interesa.\n",
        "\n",
        "<br>\n",
        "\n",
        "####**Ejemplo:**\n",
        "\n",
        "![11](https://drive.google.com/uc?export=view&id=19u_9NQlURf2hyCIP0JSiYTNVvOaS1mTA)\n",
        "\n",
        "<br>\n",
        "\n",
        "####**Atribuciones:**\n",
        "\n",
        "* Determinar que partes del input es responsable del output.\n",
        "\n",
        "<br>\n",
        "\n",
        "![11](https://drive.google.com/uc?export=view&id=1oBS-w3cJ5vpSQYQ_icO-SikqrwaUXdbl)\n",
        "\n",
        "<br>\n",
        "\n",
        "Porque hacer atribuciones? \n",
        "* Para verificar que la red no haya aprendido biases errados\n",
        "* Que no hayan atajos dañinos\n",
        "\n",
        "<br>\n",
        "\n",
        "![11](https://drive.google.com/uc?export=view&id=1OLFRPYV5p7s5HGoPtRFlNyYiLJa3AQvQ)\n",
        "\n",
        "<br>\n",
        "\n",
        "Aqui podemos apreciar que la red se esta fijando en la marca de agua para dar su veredicto, esto nos ayudará entonces a poder apreciar en que se esta fijando la red y asi diagnosticar y arreglar.\n",
        "\n",
        "<br>\n",
        "\n",
        "Hay 2 principales ramas principales de los metodos de atribución.\n",
        "* Backpropagation: modifica las reglas del backpropagation de la capa para generar una visualización del forward pass de la red.\n",
        "* Perturbaciones: cambia el input de una conducta controlada y observa una salida.\n",
        "\n",
        "<br>\n",
        "\n",
        "![12](https://drive.google.com/uc?export=view&id=1bqBLqK1KQ4iJ5joEmW2KMPTzl61yaZpC)\n",
        "\n",
        "<br>\n",
        "\n",
        "Nos enfocaremos en metodos de perturbación, particularmente en perturbaciones extremas(EP), ya que genera las mejores visualizaciones hasta el momento.\n",
        "\n",
        "<br>\n",
        "\n",
        "![13](https://drive.google.com/uc?export=view&id=1pQhIvfvYKaQgw1edzB_Yjm5cFBBPMR7T)\n",
        "\n",
        "<br>\n",
        "\n",
        "Aprende una máscara de tamaño fijo \"m\" para perturbar la entrada \"x\" que conserva al máximo la salida de la red, donde el signo fi es la red, por lo que buscaremos el m que maximice la red.\n",
        "\n",
        "\n",
        "![14](https://drive.google.com/uc?export=view&id=1nvJ3lyECws-nbCzT9m6DXW8ge4w9yWmQ)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "![15](https://drive.google.com/uc?export=view&id=1JofKdv2BlI0IYCqJBB_cbcPG4PUAiFMl)\n",
        "\n",
        "<br>\n",
        "\n",
        "La evidencia en primer plano es suficiente\n",
        "\n",
        "<br>\n",
        "\n",
        "![16](https://drive.google.com/uc?export=view&id=1xlDeqQfRtEnS2vyfucqkpZSbodgX0DMq)\n",
        "\n",
        "<br>\n",
        "\n",
        "Varios objetos contribuyen de forma acumulativa\n",
        "\n",
        "<br>\n",
        "\n",
        "![17](https://drive.google.com/uc?export=view&id=1qh8XHC61of2sVV1DUxLVsECIEiW025cM)\n",
        "\n",
        "<br>\n"
      ]
    }
  ]
}