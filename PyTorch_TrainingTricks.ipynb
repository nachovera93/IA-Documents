{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch-TrainTricks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOT/JIV/3TusDCkRgdbLyqq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nachovera93/IA-Documents/blob/main/PyTorch_TrainingTricks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxG24jLSbI29"
      },
      "source": [
        "##**Trucos de entrenamiento para CNNs + PyTorch**\n",
        "\n",
        "<br>\n",
        "\n",
        "###**Dropout**\n",
        "<br>\n",
        "\n",
        "Es una simple forma de prevenir que una red neuronal caiga en el overfitting ,osea , la red se esta aprendiendo muy bien el set de memoria pero al momento de generalizar no responderá bien. Puede mejorar de gran manera la precisión en un problema de clasificación.\n",
        "<br>\n",
        "En general, combinando diferentes tipos de modelos(Ensamble de n modelos) puede ser extremadamente utíl( Mixture of experts, majority voting, boosting, etc), pero puede ser pesado computacionalmente.\n",
        "<br>\n",
        "\n",
        "Para esto, esta el Dropout. Que consiste en apagar ciertas neuronas de una red neuronal al azar, este parametro es arbitrario. En otras palabras al apagar una neurona sería como entrenar un modelo distinto(ensamble de infinitos modelos).\n",
        "\n",
        "Lo que hace en simples palabras es que algunas neuronas no se vuelvan tan dependientes de otras neuronas. Por ejemplo, al quitar un poco de información a una imagen el modelo debería seguir resolviendo la tarea. Esto hará que estos modelos no se vuelvan locos por aprender y si por aprender patrones mas generales.\n",
        "<br>\n",
        "\n",
        "Entonces:\n",
        "<br>\n",
        "\n",
        "* Para cada iteración(época) el set de salidas para una capa de neurona oculta será cero con una cierta probabilidad.\n",
        "* Las neuronas con \"Dropped out\" de este modo no contribuirán para el forward pass y tampoco en el backpropagation.\n",
        "* Para cada input, la red neuronal tendrá diferentes arquitecturas, pero todas esas arquitecturas compartirán pesos.\n",
        "* Esta técnica reduce las coadaptaciones complejas de las neuronas, ya que cada neurona no dependerá de la presencia de otras neuronas.\n",
        " \n",
        "<br>\n",
        "Contras:\n",
        "<br>\n",
        "\n",
        "* Hace que el tiempo de entrenamiento sea mas largo al apagar neuronas. \n",
        "* Incrementa el numero de iteraciones requeridas para converger.\n",
        "\n",
        "<br>\n",
        "\n",
        "Para la clasificación no debe haber Dropout, se debe hacer solo en el entrenamiento, ya que en palabras mas simples estamos haciendole mas dificil el entrenamiento a la red para que mejore aún mas.\n",
        "\n",
        "<br>\n",
        "\n",
        "Algunos terminos y extensiones del Dropout:\n",
        "\n",
        "* DropBlock: Set entero de neuronas con Dropout.[1]\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "* DropConnect: Pesos y sesgos aleatoreamente con Dropout.[2]\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "###**Batch Normalization**\n",
        "\n",
        "<br>\n",
        "\n",
        "Le facilita a la red a la hora de aprender. Ya que por ejemplo al pasarle unos datos de entrenamiento a la red, capa por capa irán recibiendo diferentes numeros, por ende:\n",
        "\n",
        "* Hará que la distribución de los datos(rango, media, varianza, etc) de entrenamiento y de prueba coincidan. \n",
        "* Sin embargo, la distribución de la entrada a cada capa cambia durante el entrenamiento (¿por qué?). \n",
        "* Esto ralentiza el entrenamiento al requerir tasas de aprendizaje más bajas y una inicialización cuidadosa de los parámetros.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Puede reducir la tasa de aprendizaje, esto hace que el entrenamiento sea mas largo ya que los pasos que estará haciendo para actualizar nuestros parametros serán mas pequeños.\n",
        "<br>\n",
        "Entonces, lo que hará este Batch Normalization será agarrar un \"batch\" o un subconjunto del set de entrenamiento(conjunto de vectores), entonces forzará que cada una de las dimensiones de ese batch tengan una media, una varianza y los normalizará. Para dejarlos con media cero y varianza igual a 1.\n",
        "<br>\n",
        "\n",
        "![1](https://drive.google.com/uc?export=view&id=1xNLLSD6ahXf-Os-2tvN2RRS-9kmTMxbz)\n",
        "\n",
        "<br>\n",
        "\n",
        "* BN da un paso hacia la reducción del problema de desplazamiento de covariables internas entre capas.\n",
        "* Especificamente, BN introduce un paso de normalización que corrige las medias y las varianzas en capas de entrada.\n",
        "* La expectativa y la varianza se calculan sobre el mini lote para cada dimensión. Entonces cada dimensión se normaliza de manera independiente\n",
        "* Por lo tanto, BN normaliza cada característica escalar de forma independiente tratando de convertirlas en unidades gaussianas, es decir, cada dimensión tiene varianza cero y unitaria. \n",
        "\n",
        "<br>\n",
        "\n",
        "Al aplicar estas formulas y hacer 0 y 1 la media y la varianza, esto puede ser demasiado restrictivo para la red. Entonces se le agregan estos parametros (y,B) marcados en rojo, que para cada dimensión del batch se va a calcular una media aprendida y varianza aprendida para renormalizar ese batch para que quede con esos parametros y asi despúes ocuparla el batch convenientemente.\n",
        "\n",
        "\n",
        "![2](https://drive.google.com/uc?export=view&id=1IqNwUE7zVnxfAUnvv1wn8jJZOV67J3lI)\n",
        "\n",
        "<br>\n",
        "\n",
        "![3](https://drive.google.com/uc?export=view&id=111k9VKmZVf0QiTB6zSTG1Llu0qbIBKz-)\n",
        "\n",
        "<br>\n",
        "\n",
        "* La simple normalización de cada entrada de una capa puede cambiar el espacio de representación de la capa. \n",
        "* Normalizar las entradas de una sigmoide las restringiría a permanecer en la parte lineal del sigmoide.\n",
        "* Necesitamos agregar un mecanismo para compensar este efecto.\n",
        "* BN introduce una transformación que, de ser necesario, permite a la red cancelar la operación del operador BN.\n",
        "* Es decir, la transformación le da a la red la flexibilidad de convertir al operador BN en la función de identidad. \n",
        "\n",
        "<br>\n",
        "\n",
        "![1a](https://drive.google.com/uc?export=view&id=1JyIBe00-bQMB44apefbpLfF9J2tlfvGL)\n",
        "\n",
        "<br> \n",
        "\n",
        "Entonces como se muestra en las ecuaciones anteriores, Aplicamos normalización y aplicamos parametros para desplazar la media y cambiar la varianza.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Usualmente ubicamos el BN en las siguientes capas:\n",
        "<br>\n",
        "\n",
        "![1c](https://drive.google.com/uc?export=view&id=1Ofwjx00MNbUYwXI47c_-zAUtFh53DYKR)\n",
        "\n",
        "<br>\n",
        "\n",
        "![2d](https://drive.google.com/uc?export=view&id=1mG17zV39aOPq2OadzwScmYKojRDnyk_n)\n",
        "\n",
        "<br>\n",
        "\n",
        "Como sabemos el BN reduce el tiempo de convergencia de una red y le permite aprender mas rapido a esta. Aqui podemos apreciar un test de precisión de MNIST con BN y sin BN.\n",
        "<br>\n",
        "En el grafico (a) los valores de 10k-50k corresponden a la cantidad de iteraciones o actualizaciones de gradiente\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**El Conjunto de todos los Batches hacen una epoca**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJo9ZDgcGAeS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khmb1jCHK3Lr"
      },
      "source": [
        "[1]https://arxiv.org/abs/1810.12890\n",
        "<br>\n",
        "[2]http://yann.lecun.com/exdb/publis/pdf/wan-icml-13.pdf"
      ]
    }
  ]
}